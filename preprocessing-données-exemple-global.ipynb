{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb3cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    [22,1800,\"Paris\",\"CDD\",1],\n",
    "    [35,3200,\"Lyon\",\"CDI\",0],\n",
    "    [29,2500,\"Marseille\",\"CDD\",1],\n",
    "    [41,4200,\"Toulouse\",\"CDI\",0],\n",
    "    [50,5200,\"Paris\",\"CDI\",0],\n",
    "    [27,2300,\"Lille\",\"CDD\",1],\n",
    "    [33,3100,\"Nantes\",\"CDI\",0],\n",
    "    [45,4800,\"Bordeaux\",\"CDI\",0],\n",
    "    [38,3600,\"Lyon\",\"CDI\",0],\n",
    "    [24,1900,\"Marseille\",\"CDD\",1],\n",
    "    [31,2900,\"Toulouse\",\"CDD\",1],\n",
    "    [54,6000,\"Paris\",\"CDI\",0],\n",
    "    [47,5100,\"Bordeaux\",\"CDI\",0],\n",
    "    [26,2100,\"Lille\",\"CDD\",1],\n",
    "    [39,3700,\"Nantes\",\"CDI\",0],\n",
    "    [28,2400,\"Lyon\",\"CDD\",1],\n",
    "    [43,4500,\"Marseille\",\"CDI\",0],\n",
    "    [36,3400,\"Toulouse\",\"CDI\",0],\n",
    "    [52,5800,\"Paris\",\"CDI\",0],\n",
    "    [23,1850,\"Nantes\",\"CDD\",1],\n",
    "    [34,3000,\"Lille\",\"CDI\",0],\n",
    "    [48,5300,\"Bordeaux\",\"CDI\",0],\n",
    "    [40,4000,\"Lyon\",\"CDI\",0],\n",
    "    [30,2700,\"Marseille\",\"CDD\",1],\n",
    "    [55,6200,\"Paris\",\"CDI\",0],\n",
    "    [37,3500,\"Toulouse\",\"CDI\",0],\n",
    "    [25,2000,\"Lille\",\"CDD\",1],\n",
    "    [42,4600,\"Nantes\",\"CDI\",0],\n",
    "    [32,2950,\"Bordeaux\",\"CDI\",0],\n",
    "    [29,2600,\"Lyon\",\"CDD\",1],\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\"age\", \"revenu\", \"ville\", \"type_contrat\", \"churn\"]\n",
    ")\n",
    "\n",
    "df.to_csv(\"data/clients.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd18276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu des données brutes :\n",
      "   age  revenu      ville type_contrat  churn\n",
      "0   22    1800      Paris          CDD      1\n",
      "1   35    3200       Lyon          CDI      0\n",
      "2   29    2500  Marseille          CDD      1\n",
      "3   41    4200   Toulouse          CDI      0\n",
      "4   50    5200      Paris          CDI      0\n",
      "Colonnes numériques : ['age', 'revenu']\n",
      "Colonnes catégorielles : ['ville', 'type_contrat']\n",
      "\n",
      "Shapes après preprocessing :\n",
      "X_train : (21, 11)\n",
      "X_val   : (3, 11)\n",
      "X_test  : (6, 11)\n",
      "y_train : (21,)\n",
      "y_val   : (3,)\n",
      "y_test  : (6,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "preprocess_tabular.py\n",
    "=====================\n",
    "\n",
    "Exemples de prétraitement de données tabulaires :\n",
    "- Chargement d'un CSV avec pandas\n",
    "- Nettoyage basique (valeurs manquantes, doublons)\n",
    "- Séparation features / cible\n",
    "- Différentes méthodes de preprocessing :\n",
    "    * Imputation (moyenne / médiane / plus fréquent)\n",
    "    * Standardisation (StandardScaler)\n",
    "    * Normalisation Min-Max (MinMaxScaler)\n",
    "    * Encodage One-Hot pour les variables catégorielles\n",
    "- Split train / validation / test\n",
    "\n",
    "Exécution de démonstration dans le main.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Chargement et nettoyage basique\n",
    "# ============================================================\n",
    "\n",
    "def load_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Charge un fichier CSV dans un DataFrame pandas.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        Chemin vers le fichier CSV.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame contenant les données chargées.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def basic_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Nettoyage basique des données :\n",
    "    - Supprime les doublons\n",
    "    - (Option) supprime des lignes complètement vides, si besoin\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Données brutes.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    df_clean : pd.DataFrame\n",
    "        Données nettoyées.\n",
    "    \"\"\"\n",
    "    # Supprimer les doublons (mêmes lignes)\n",
    "    df_clean = df.drop_duplicates()\n",
    "\n",
    "    # Exemple : supprimer les lignes 100% NaN (optionnel)\n",
    "    df_clean = df_clean.dropna(how=\"all\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Fonctions utilitaires de prétraitement\n",
    "# ============================================================\n",
    "\n",
    "def get_feature_target(df: pd.DataFrame, target_col: str):\n",
    "    \"\"\"\n",
    "    Sépare les features (X) et la variable cible (y).\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Données nettoyées.\n",
    "    target_col : str\n",
    "        Nom de la colonne cible.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    X : pd.DataFrame\n",
    "        Variables explicatives.\n",
    "    y : pd.Series\n",
    "        Variable à prédire.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def detect_column_types(X: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Détecte automatiquement les colonnes numériques et catégorielles.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Variables explicatives.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    numeric_cols : list\n",
    "        Noms des colonnes numériques.\n",
    "    categorical_cols : list\n",
    "        Noms des colonnes catégorielles (object / string).\n",
    "    \"\"\"\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Différentes méthodes de preprocessing\n",
    "#    (imputation, scaling, encodage)\n",
    "# ============================================================\n",
    "\n",
    "def build_numeric_pipeline(strategy_impute: str = \"median\",\n",
    "                           scaling: str = \"standard\") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Construit un pipeline de prétraitement pour les colonnes numériques.\n",
    "\n",
    "    - Imputation des valeurs manquantes :\n",
    "        * \"mean\"   : moyenne\n",
    "        * \"median\" : médiane\n",
    "        * \"most_frequent\" : valeur la plus fréquente\n",
    "    - Scaling :\n",
    "        * \"standard\" : StandardScaler (moyenne 0, écart-type 1)\n",
    "        * \"minmax\"   : MinMaxScaler (entre 0 et 1)\n",
    "        * None       : pas de scaling\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    strategy_impute : str\n",
    "        Stratégie d'imputation des NaN.\n",
    "    scaling : str\n",
    "        Type de normalisation / standardisation.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    num_pipeline : Pipeline\n",
    "        Pipeline sklearn pour les colonnes numériques.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "\n",
    "    # Étape 1 : Imputation des valeurs manquantes\n",
    "    steps.append(\n",
    "        (\"imputer\", SimpleImputer(strategy=strategy_impute))\n",
    "    )\n",
    "\n",
    "    # Étape 2 : Scaling (optionnel)\n",
    "    if scaling == \"standard\":\n",
    "        steps.append((\"scaler\", StandardScaler()))\n",
    "    elif scaling == \"minmax\":\n",
    "        steps.append((\"scaler\", MinMaxScaler()))\n",
    "    else:\n",
    "        # Aucun scaling si scaling == None\n",
    "        pass\n",
    "\n",
    "    num_pipeline = Pipeline(steps)\n",
    "    return num_pipeline\n",
    "\n",
    "\n",
    "def build_categorical_pipeline(handle_unknown: str = \"ignore\") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Construit un pipeline pour les colonnes catégorielles.\n",
    "\n",
    "    Étapes :\n",
    "    - Imputation de la valeur manquante par la catégorie la plus fréquente.\n",
    "    - Encodage One-Hot (création de colonnes binaires 0/1).\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    handle_unknown : str\n",
    "        Comportement si une catégorie inconnue apparaît en prédiction.\n",
    "        \"ignore\" est souvent utilisé pour éviter les erreurs.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    cat_pipeline : Pipeline\n",
    "        Pipeline sklearn pour les colonnes catégorielles.\n",
    "    \"\"\"\n",
    "    cat_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=handle_unknown))\n",
    "        ]\n",
    "    )\n",
    "    return cat_pipeline\n",
    "\n",
    "\n",
    "def build_preprocessor(X: pd.DataFrame,\n",
    "                       strategy_impute_num: str = \"median\",\n",
    "                       scaling_num: str = \"standard\") -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Crée un préprocesseur complet (ColumnTransformer) combinant :\n",
    "    - Pipeline numérique\n",
    "    - Pipeline catégoriel\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Données d'entrée (features uniquement).\n",
    "    strategy_impute_num : str\n",
    "        Stratégie d'imputation pour les colonnes numériques.\n",
    "    scaling_num : str\n",
    "        Type de scaling pour les colonnes numériques.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    preprocessor : ColumnTransformer\n",
    "        Objet sklearn qui applique les bons traitements aux bonnes colonnes.\n",
    "    numeric_cols : list\n",
    "    categorical_cols : list\n",
    "        Listes des noms de colonnes pour information.\n",
    "    \"\"\"\n",
    "    numeric_cols, categorical_cols = detect_column_types(X)\n",
    "\n",
    "    num_pipeline = build_numeric_pipeline(\n",
    "        strategy_impute=strategy_impute_num,\n",
    "        scaling=scaling_num\n",
    "    )\n",
    "    cat_pipeline = build_categorical_pipeline()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipeline, numeric_cols),\n",
    "            (\"cat\", cat_pipeline, categorical_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor, numeric_cols, categorical_cols\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Split train / validation / test + application du preprocessing\n",
    "# ============================================================\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Sépare les données en train / validation / test.\n",
    "\n",
    "    - On commence par séparer test.\n",
    "    - Puis on coupe le train restant en train / val.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "    y : pd.Series\n",
    "    test_size : float\n",
    "        Proportion pour le test (ex : 0.2 = 20 %).\n",
    "    val_size : float\n",
    "        Proportion pour la validation par rapport au total (ex : 0.1 = 10 %).\n",
    "    random_state : int\n",
    "        Graines aléatoire pour reproductibilité.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    # 1) Split train+val / test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # 2) Calcul de la part validation sur le train_val\n",
    "    val_ratio = val_size / (1.0 - test_size)\n",
    "\n",
    "    # 3) Split train / val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val,\n",
    "        test_size=val_ratio,\n",
    "        random_state=random_state,\n",
    "        stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def preprocess_dataset(df: pd.DataFrame,\n",
    "                       target_col: str,\n",
    "                       strategy_impute_num: str = \"median\",\n",
    "                       scaling_num: str = \"standard\"):\n",
    "    \"\"\"\n",
    "    Fonction \"tout-en-un\" :\n",
    "\n",
    "    1. Nettoyage basique\n",
    "    2. Séparation X / y\n",
    "    3. Split train / val / test\n",
    "    4. Construction du préprocesseur\n",
    "    5. Fit du préprocesseur sur le train puis transformation de tous les splits\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Données brutes.\n",
    "    target_col : str\n",
    "        Nom de la colonne cible.\n",
    "    strategy_impute_num : str\n",
    "        Stratégie d'imputation numérique.\n",
    "    scaling_num : str\n",
    "        Type de scaling numérique.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    X_train_p, X_val_p, X_test_p : np.ndarray\n",
    "        Features prétraitées (prêtes pour un modèle).\n",
    "    y_train, y_val, y_test : pd.Series\n",
    "        Cible pour chaque split.\n",
    "    preprocessor : ColumnTransformer\n",
    "        Objet préprocesseur entraîné (pour réutilisation / sauvegarde).\n",
    "    \"\"\"\n",
    "    # Nettoyage\n",
    "    df_clean = basic_cleaning(df)\n",
    "\n",
    "    # Séparation features / cible\n",
    "    X, y = get_feature_target(df_clean, target_col=target_col)\n",
    "\n",
    "    # Split train / val / test\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "\n",
    "    # Construction du préprocesseur\n",
    "    preprocessor, num_cols, cat_cols = build_preprocessor(\n",
    "        X_train,\n",
    "        strategy_impute_num=strategy_impute_num,\n",
    "        scaling_num=scaling_num\n",
    "    )\n",
    "\n",
    "    print(\"Colonnes numériques :\", num_cols)\n",
    "    print(\"Colonnes catégorielles :\", cat_cols)\n",
    "\n",
    "    # Fit du préprocesseur sur le train, puis transformation\n",
    "    X_train_p = preprocessor.fit_transform(X_train)\n",
    "    X_val_p = preprocessor.transform(X_val)\n",
    "    X_test_p = preprocessor.transform(X_test)\n",
    "\n",
    "    return X_train_p, X_val_p, X_test_p, y_train, y_val, y_test, preprocessor\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Exemple d'utilisation dans un main\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Exemple concret :\n",
    "\n",
    "    Supposons un fichier 'data/clients.csv' avec une colonne cible 'churn'\n",
    "    (0/1 : le client quitte ou non le service).\n",
    "\n",
    "    Colonnes possibles :\n",
    "    - age (numérique)\n",
    "    - revenu (numérique)\n",
    "    - ville (catégorielle)\n",
    "    - type_contrat (catégorielle)\n",
    "    - churn (cible binaire)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Chemin vers le CSV (à adapter à votre cas)\n",
    "    csv_path = \"data/clients.csv\"\n",
    "\n",
    "    # 2) Chargement des données\n",
    "    try:\n",
    "        df = load_data(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Fichier non trouvé : {csv_path}\")\n",
    "        print(\"Créez un CSV d'exemple ou changez le chemin.\")\n",
    "        exit(1)\n",
    "\n",
    "    print(\"Aperçu des données brutes :\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 3) Prétraitement complet\n",
    "    X_train_p, X_val_p, X_test_p, y_train, y_val, y_test, preprocessor = preprocess_dataset(\n",
    "        df,\n",
    "        target_col=\"churn\",          # ⚠️ à adapter au nom de votre colonne cible\n",
    "        strategy_impute_num=\"median\",\n",
    "        scaling_num=\"standard\"       # ou \"minmax\" ou None\n",
    "    )\n",
    "\n",
    "    # 4) Affichage des shapes finales\n",
    "    print(\"\\nShapes après preprocessing :\")\n",
    "    print(\"X_train :\", X_train_p.shape)\n",
    "    print(\"X_val   :\", X_val_p.shape)\n",
    "    print(\"X_test  :\", X_test_p.shape)\n",
    "    print(\"y_train :\", y_train.shape)\n",
    "    print(\"y_val   :\", y_val.shape)\n",
    "    print(\"y_test  :\", y_test.shape)\n",
    "\n",
    "    # Ici vous pouvez ensuite entraîner un modèle, par ex :\n",
    "    # from sklearn.linear_model import LogisticRegression\n",
    "    # model = LogisticRegression(max_iter=1000)\n",
    "    # model.fit(X_train_p, y_train)\n",
    "    # print(\"Score val :\", model.score(X_val_p, y_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
